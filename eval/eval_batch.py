#!/usr/bin/env python3

import os
import json
from glob import glob
from pathlib import Path
from eval_all import PoseEvaluator
import csv
import datetime

DATASETS_ROOT = "/media/wby/6d811df4-bde7-479b-ab4c-679222653ea0/dataset_done_multi"
META_FILE = "eval_objects_multi.json"     # ç»Ÿä¸€metaæ–‡ä»¶
OUT_SUMMARY_FILE = "batch_eval_summary_multi.json"


# -----------------------------------------------------------
# 1. object meta ç»Ÿä¸€åŠ è½½
# -----------------------------------------------------------

def load_global_object_meta(meta_path):
    if not os.path.exists(meta_path):
        raise FileNotFoundError(f"ç»Ÿä¸€metaæ–‡ä»¶ä¸å­˜åœ¨: {meta_path}")

    with open(meta_path, "r") as f:
        return json.load(f)


# -----------------------------------------------------------
# 2. ä» eval æ–‡ä»¶å‘ç°å½“å‰ dataset çš„ object_id
# -----------------------------------------------------------

def auto_find_object_ids(dataset_dir):
    ids = []
    for f in glob(os.path.join(dataset_dir, "eval", "object_*.txt")):
        name = os.path.basename(f)
        num = name.replace("object_", "").replace(".txt", "")
        if num.isdigit():
            ids.append(int(num))
    return sorted(ids)

def append_global_csv(csv_path, row_dict, fieldnames):
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    file_exists = os.path.exists(csv_path)

    with open(csv_path, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerow(row_dict)



# -----------------------------------------------------------
# 3. ä¸»é©±åŠ¨ï¼šå®Œå…¨ç”± JSON æ§åˆ¶
# -----------------------------------------------------------

def main():

    print("\n======= æ‰¹é‡è¯„æµ‹å¯åŠ¨ï¼ˆJSON ä¸¥æ ¼æ§åˆ¶æ¨¡å¼ï¼‰ =======")

    meta = load_global_object_meta(META_FILE)

    if not meta:
        print("[ERROR] eval_objects.json å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œè¯„æµ‹")
        return

    all_results = []

    for dataset_name, obj_dict in meta.items():

        dataset_dir = os.path.join(DATASETS_ROOT, dataset_name)

        print("\n" + "=" * 68)
        print(f"Dataset: {dataset_name}")

        # ---------- dataset ä¸å­˜åœ¨ ----------
        if not os.path.isdir(dataset_dir):
            print(f"[WARN] dataset ä¸å­˜åœ¨, skip: {dataset_dir}")
            continue

        # ---------- å¿…è¦å­ç›®å½•æ£€æŸ¥ ----------
        if not (Path(dataset_dir) / "eval").is_dir() or \
           not (Path(dataset_dir) / "pose_txt").is_dir():
            print("[WARN] dataset ç¼ºå°‘å¿…è¦ç›®å½•(eval/pose_txt)ï¼Œskip")
            continue

        evaluator = PoseEvaluator(dataset_dir)

        # æ‰«æç£ç›˜å®é™…å­˜åœ¨çš„ object_id
        disk_object_ids = auto_find_object_ids(dataset_dir)
        print(f"ç£ç›˜ä¸­å‘ç° object_id: {disk_object_ids}")

        for sid, obj_name in obj_dict.items():

            oid = int(sid)

            print(f"\n--- Evaluating [{dataset_name}] Object id={oid}, name={obj_name}")

            # ---------- eval æ–‡ä»¶æ£€æŸ¥ ----------
            eval_file = os.path.join(dataset_dir, "eval", f"object_{oid}.txt")

            if not os.path.exists(eval_file):
                print(f"[WARN] æ‰¾ä¸åˆ° {eval_file}, skip")
                continue

            try:
                evaluator.evaluate(
                    object_id=oid,
                    object_name=obj_name
                )

                res_file = os.path.join(
                    dataset_dir,
                    "eval",
                    "evaluation_results.json"
                )

                if os.path.exists(res_file):
                    with open(res_file) as f:
                        res = json.load(f)
                else:
                    res = None

                all_results.append({
                    "dataset": dataset_name,
                    "object_id": oid,
                    "object_name": obj_name,
                    "results": res
                })

            except Exception as e:
                print(f"[ERROR] å¤±è´¥: {e}")
                all_results.append({
                    "dataset": dataset_name,
                    "object_id": oid,
                    "object_name": obj_name,
                    "error": str(e)
                })


    # -----------------------------------------------------------
    # 4. ä¿å­˜å…¨å±€æ±‡æ€»
    # -----------------------------------------------------------

    out_path = os.path.join(os.path.dirname(__file__), OUT_SUMMARY_FILE)

    with open(out_path, "w") as f:
        json.dump(all_results, f, indent=2)

    print("--------------------------------")
    print(f"ğŸ“„ æ±‡æ€»æ–‡ä»¶å†™å…¥: {out_path}")


if __name__ == "__main__":
    main()
